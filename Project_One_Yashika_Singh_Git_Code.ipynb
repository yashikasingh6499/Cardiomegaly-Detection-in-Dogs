{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "009e8fc2",
      "metadata": {
        "id": "009e8fc2"
      },
      "source": [
        "# 1. Build your own convolutional neural network using pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133be475",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "133be475",
        "outputId": "048e7e55-2282-404a-b287-23ed4ec7648a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Defining a custom convolutional neural network (CNN)\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        # Custom CNN layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passing through custom CNN layers\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 4\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "\n",
        "# Creating synthetic data for training (e.g., random tensors)\n",
        "x_train = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 image size\n",
        "y_train = torch.randint(0, num_classes, (100,))  # 100 labels for 4 classes\n",
        "\n",
        "# Creating DataLoader\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initializing model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomCNN(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Computing training statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c45b84",
      "metadata": {
        "id": "a0c45b84"
      },
      "source": [
        "# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code)\n",
        "\n",
        "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
        "### (2) use custom dataloader for test dataset (return image tensor and file name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50effdac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50effdac",
        "outputId": "1c190feb-ea41-4c76-cd9b-c5c8b65a996e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100:\n",
            "Train Loss: 23.3761, Train Acc: 37.07%\n",
            "Valid Loss: 2.6390, Valid Acc: 45.50%\n",
            "Epoch 2/100:\n",
            "Train Loss: 2.4847, Train Acc: 40.14%\n",
            "Valid Loss: 1.3741, Valid Acc: 44.00%\n",
            "Epoch 3/100:\n",
            "Train Loss: 1.1202, Train Acc: 47.64%\n",
            "Valid Loss: 1.0504, Valid Acc: 54.00%\n",
            "Epoch 4/100:\n",
            "Train Loss: 0.8180, Train Acc: 55.71%\n",
            "Valid Loss: 0.8375, Valid Acc: 61.00%\n",
            "Epoch 5/100:\n",
            "Train Loss: 0.7797, Train Acc: 56.86%\n",
            "Valid Loss: 0.6626, Valid Acc: 54.50%\n",
            "Epoch 6/100:\n",
            "Train Loss: 0.7475, Train Acc: 60.29%\n",
            "Valid Loss: 0.6318, Valid Acc: 66.00%\n",
            "Epoch 7/100:\n",
            "Train Loss: 0.7046, Train Acc: 63.50%\n",
            "Valid Loss: 0.6012, Valid Acc: 63.50%\n",
            "Epoch 8/100:\n",
            "Train Loss: 0.6733, Train Acc: 65.57%\n",
            "Valid Loss: 0.6885, Valid Acc: 61.50%\n",
            "Epoch 9/100:\n",
            "Train Loss: 0.6110, Train Acc: 68.93%\n",
            "Valid Loss: 0.6080, Valid Acc: 66.50%\n",
            "Epoch 10/100:\n",
            "Train Loss: 0.5911, Train Acc: 71.43%\n",
            "Valid Loss: 0.5846, Valid Acc: 67.00%\n",
            "Epoch 11/100:\n",
            "Train Loss: 0.6043, Train Acc: 69.79%\n",
            "Valid Loss: 0.5939, Valid Acc: 66.50%\n",
            "Epoch 12/100:\n",
            "Train Loss: 0.7623, Train Acc: 63.93%\n",
            "Valid Loss: 0.6116, Valid Acc: 67.00%\n",
            "Epoch 13/100:\n",
            "Train Loss: 0.7686, Train Acc: 59.57%\n",
            "Valid Loss: 1.9002, Valid Acc: 52.50%\n",
            "Epoch 14/100:\n",
            "Train Loss: 0.7487, Train Acc: 59.86%\n",
            "Valid Loss: 0.7728, Valid Acc: 66.00%\n",
            "Epoch 15/100:\n",
            "Train Loss: 0.6734, Train Acc: 65.64%\n",
            "Valid Loss: 0.6144, Valid Acc: 68.50%\n",
            "Epoch 16/100:\n",
            "Train Loss: 0.6133, Train Acc: 67.64%\n",
            "Valid Loss: 0.6403, Valid Acc: 70.50%\n",
            "Epoch 17/100:\n",
            "Train Loss: 0.6133, Train Acc: 68.43%\n",
            "Valid Loss: 1.0295, Valid Acc: 59.00%\n",
            "Epoch 18/100:\n",
            "Train Loss: 0.5834, Train Acc: 70.64%\n",
            "Valid Loss: 0.5881, Valid Acc: 70.50%\n",
            "Epoch 19/100:\n",
            "Train Loss: 0.5380, Train Acc: 73.21%\n",
            "Valid Loss: 0.8462, Valid Acc: 62.50%\n",
            "Epoch 20/100:\n",
            "Train Loss: 0.5257, Train Acc: 74.14%\n",
            "Valid Loss: 0.6662, Valid Acc: 70.50%\n",
            "Epoch 21/100:\n",
            "Train Loss: 0.5137, Train Acc: 75.07%\n",
            "Valid Loss: 0.7173, Valid Acc: 67.50%\n",
            "Epoch 22/100:\n",
            "Train Loss: 0.6242, Train Acc: 69.21%\n",
            "Valid Loss: 0.5957, Valid Acc: 67.00%\n",
            "Epoch 23/100:\n",
            "Train Loss: 0.5890, Train Acc: 69.57%\n",
            "Valid Loss: 0.6233, Valid Acc: 67.00%\n",
            "Epoch 24/100:\n",
            "Train Loss: 0.5899, Train Acc: 72.36%\n",
            "Valid Loss: 0.6122, Valid Acc: 70.50%\n",
            "Epoch 25/100:\n",
            "Train Loss: 0.6063, Train Acc: 70.50%\n",
            "Valid Loss: 0.6976, Valid Acc: 60.50%\n",
            "Epoch 26/100:\n",
            "Train Loss: 0.5714, Train Acc: 74.00%\n",
            "Valid Loss: 0.9658, Valid Acc: 59.00%\n",
            "Epoch 27/100:\n",
            "Train Loss: 0.5279, Train Acc: 76.57%\n",
            "Valid Loss: 0.5644, Valid Acc: 71.00%\n",
            "Epoch 28/100:\n",
            "Train Loss: 0.4950, Train Acc: 77.00%\n",
            "Valid Loss: 0.5404, Valid Acc: 73.00%\n",
            "Epoch 29/100:\n",
            "Train Loss: 0.4392, Train Acc: 79.50%\n",
            "Valid Loss: 0.5564, Valid Acc: 73.00%\n",
            "Epoch 30/100:\n",
            "Train Loss: 0.4488, Train Acc: 79.29%\n",
            "Valid Loss: 0.5886, Valid Acc: 71.50%\n",
            "Epoch 31/100:\n",
            "Train Loss: 0.4496, Train Acc: 79.86%\n",
            "Valid Loss: 0.5748, Valid Acc: 73.00%\n",
            "Epoch 32/100:\n",
            "Train Loss: 0.5583, Train Acc: 75.00%\n",
            "Valid Loss: 0.8459, Valid Acc: 66.50%\n",
            "Epoch 33/100:\n",
            "Train Loss: 0.5895, Train Acc: 71.57%\n",
            "Valid Loss: 0.5998, Valid Acc: 67.00%\n",
            "Epoch 34/100:\n",
            "Train Loss: 0.5596, Train Acc: 75.00%\n",
            "Valid Loss: 0.6532, Valid Acc: 67.00%\n",
            "Epoch 35/100:\n",
            "Train Loss: 0.5280, Train Acc: 75.86%\n",
            "Valid Loss: 0.5408, Valid Acc: 73.50%\n",
            "Epoch 36/100:\n",
            "Train Loss: 0.4973, Train Acc: 76.86%\n",
            "Valid Loss: 0.5419, Valid Acc: 70.00%\n",
            "Epoch 37/100:\n",
            "Train Loss: 0.4397, Train Acc: 80.00%\n",
            "Valid Loss: 0.6391, Valid Acc: 70.00%\n",
            "Epoch 38/100:\n",
            "Train Loss: 0.4351, Train Acc: 80.50%\n",
            "Valid Loss: 0.5856, Valid Acc: 72.50%\n",
            "Epoch 39/100:\n",
            "Train Loss: 0.4406, Train Acc: 80.07%\n",
            "Valid Loss: 0.6236, Valid Acc: 74.00%\n",
            "Epoch 40/100:\n",
            "Train Loss: 0.3937, Train Acc: 82.79%\n",
            "Valid Loss: 0.6007, Valid Acc: 72.00%\n",
            "Epoch 41/100:\n",
            "Train Loss: 0.3729, Train Acc: 83.93%\n",
            "Valid Loss: 0.5951, Valid Acc: 72.00%\n",
            "Epoch 42/100:\n",
            "Train Loss: 0.5539, Train Acc: 75.79%\n",
            "Valid Loss: 0.6651, Valid Acc: 69.50%\n",
            "Epoch 43/100:\n",
            "Train Loss: 0.5355, Train Acc: 74.79%\n",
            "Valid Loss: 0.8317, Valid Acc: 66.00%\n",
            "Epoch 44/100:\n",
            "Train Loss: 0.5224, Train Acc: 77.36%\n",
            "Valid Loss: 0.6331, Valid Acc: 66.50%\n",
            "Epoch 45/100:\n",
            "Train Loss: 0.4606, Train Acc: 79.64%\n",
            "Valid Loss: 0.7208, Valid Acc: 65.50%\n",
            "Epoch 46/100:\n",
            "Train Loss: 0.4237, Train Acc: 81.14%\n",
            "Valid Loss: 0.5496, Valid Acc: 73.00%\n",
            "Epoch 47/100:\n",
            "Train Loss: 0.4231, Train Acc: 82.14%\n",
            "Valid Loss: 0.5446, Valid Acc: 74.50%\n",
            "Epoch 48/100:\n",
            "Train Loss: 0.3985, Train Acc: 83.00%\n",
            "Valid Loss: 0.5933, Valid Acc: 74.50%\n",
            "Epoch 49/100:\n",
            "Train Loss: 0.3821, Train Acc: 83.79%\n",
            "Valid Loss: 0.5375, Valid Acc: 76.00%\n",
            "Epoch 50/100:\n",
            "Train Loss: 0.3212, Train Acc: 87.14%\n",
            "Valid Loss: 0.5372, Valid Acc: 76.00%\n",
            "Epoch 51/100:\n",
            "Train Loss: 0.3431, Train Acc: 85.29%\n",
            "Valid Loss: 0.5299, Valid Acc: 75.00%\n",
            "Epoch 52/100:\n",
            "Train Loss: 0.4363, Train Acc: 81.14%\n",
            "Valid Loss: 0.7260, Valid Acc: 66.50%\n",
            "Epoch 53/100:\n",
            "Train Loss: 0.5585, Train Acc: 75.29%\n",
            "Valid Loss: 0.6024, Valid Acc: 68.00%\n",
            "Epoch 54/100:\n",
            "Train Loss: 0.4870, Train Acc: 77.57%\n",
            "Valid Loss: 0.6152, Valid Acc: 70.00%\n",
            "Epoch 55/100:\n",
            "Train Loss: 0.4543, Train Acc: 81.43%\n",
            "Valid Loss: 0.5453, Valid Acc: 74.50%\n",
            "Epoch 56/100:\n",
            "Train Loss: 0.4106, Train Acc: 82.43%\n",
            "Valid Loss: 0.5249, Valid Acc: 74.50%\n",
            "Epoch 57/100:\n",
            "Train Loss: 0.3584, Train Acc: 84.29%\n",
            "Valid Loss: 0.6359, Valid Acc: 74.00%\n",
            "Epoch 58/100:\n",
            "Train Loss: 0.3550, Train Acc: 84.93%\n",
            "Valid Loss: 0.5586, Valid Acc: 74.00%\n",
            "Epoch 59/100:\n",
            "Train Loss: 0.3240, Train Acc: 86.14%\n",
            "Valid Loss: 0.6427, Valid Acc: 73.00%\n",
            "Epoch 60/100:\n",
            "Train Loss: 0.3203, Train Acc: 85.86%\n",
            "Valid Loss: 0.5625, Valid Acc: 75.00%\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-1dfb93c38288>:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  custom_cnn.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to test_results.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import csv\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "\n",
        "# Defining the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Extracting zip files\n",
        "zip_files = {'Dog_heart.zip': 'Dog_heart', 'Test.zip': 'Test'}\n",
        "for zip_file, extract_path in zip_files.items():\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# Enhanced data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_valid = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Data loading with improved batch size\n",
        "train_data_path = 'Dog_heart/Dog_heart/Train'\n",
        "valid_data_path = 'Dog_heart/Dog_heart/Valid'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform_train)\n",
        "valid_dataset = datasets.ImageFolder(root=valid_data_path, transform=transform_valid)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, os.path.basename(img_path)\n",
        "\n",
        "# Test data loading\n",
        "test_data_path = 'Test/Test'\n",
        "test_dataset = CustomTestDataset(root_dir=test_data_path, transform=transform_valid)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Defining a custom CNN model\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        # Custom CNN layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Pretrained GoogleNet for feature extraction\n",
        "        self.googlenet = models.googlenet(pretrained=True)\n",
        "        self.googlenet.fc = nn.Linear(in_features=1024, out_features=4)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14 + 4, 512)  # Combining custom CNN and GoogleNet features\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passing through custom CNN layers\n",
        "        x_custom = F.relu(self.bn1(self.conv1(x)))\n",
        "        x_custom = self.pool(x_custom)\n",
        "        x_custom = F.relu(self.bn2(self.conv2(x_custom)))\n",
        "        x_custom = self.pool(x_custom)\n",
        "        x_custom = F.relu(self.bn3(self.conv3(x_custom)))\n",
        "        x_custom = self.pool(x_custom)\n",
        "        x_custom = F.relu(self.bn4(self.conv4(x_custom)))\n",
        "        x_custom = self.pool(x_custom)\n",
        "        x_custom = self.dropout(x_custom)\n",
        "        x_custom = torch.flatten(x_custom, 1)\n",
        "\n",
        "        # Passing through GoogleNet for feature extraction\n",
        "        x_google = self.googlenet(x)\n",
        "        x_google = torch.flatten(x_google, 1)\n",
        "\n",
        "        # Concatenating features from custom CNN and GoogleNet\n",
        "        x = torch.cat((x_custom, x_google), dim=1)\n",
        "\n",
        "        # Passing through fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initializing the custom CNN model\n",
        "custom_cnn = CustomCNN(num_classes=4).to(device)\n",
        "\n",
        "# Initializing loss, optimizer, and schedulers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(custom_cnn.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Learning rate scheduler with warm-up and ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
        "warmup_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
        "plateau_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Early Stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_acc, model):\n",
        "        score = val_acc\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Training function with mixed precision\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device_type='cuda'):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(train_loader), 100 * correct / total\n",
        "\n",
        "# Validation function\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return val_loss / len(valid_loader), 100 * correct / total\n",
        "\n",
        "# Training loop with early stopping\n",
        "num_epochs = 100\n",
        "best_acc = 0\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(custom_cnn, train_loader, criterion, optimizer, scaler)\n",
        "    val_loss, val_acc = validate(custom_cnn, valid_loader, criterion)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Valid Loss: {val_loss:.4f}, Valid Acc: {val_acc:.2f}%')\n",
        "\n",
        "    warmup_scheduler.step(epoch + np.random.rand())\n",
        "    plateau_scheduler.step(val_acc)\n",
        "\n",
        "    early_stopping(val_acc, custom_cnn)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# Loading best model for testing\n",
        "custom_cnn.load_state_dict(torch.load('best_model.pth'))\n",
        "custom_cnn.eval()\n",
        "\n",
        "# Testing and saving results\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for inputs, file_names in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = custom_cnn(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        for file_name, pred in zip(file_names, predicted):\n",
        "            results.append({'Image': file_name, 'Predicted Class': pred.item()})\n",
        "\n",
        "# Saving results to CSV\n",
        "csv_file_path = 'test_results.csv'\n",
        "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for result in results:\n",
        "        writer.writerow([result['Image'], result['Predicted Class']])\n",
        "\n",
        "print(f'Results saved to {csv_file_path}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}